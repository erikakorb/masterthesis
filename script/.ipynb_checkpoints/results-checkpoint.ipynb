{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220774aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries ###\n",
    "import os\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import WRBH_select    # WRBH_select.py, in the same folder, to select subset of binaries from WRBH df\n",
    "import read           # read.py, to reduce the original output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62e11143",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters ###\n",
    "# [version] is the SEVN2 version adopted                            e.g. '3.0.0-Spindevel'\n",
    "# [Nsim] is the number of simulations                               e.g. '1mln'\n",
    "# [Z] is the metallicity of the stars                               e.g. '02' for Z=0.02 solar metallicity\n",
    "# [SN] is the SN explosion prescription adopted                     e.g. 'del' for delayed or 'com' for compact\n",
    "# [n] number of cores involved in dask computation                  e.g.  4\n",
    "# [Hsup] is the threshold for spectroscopic WR                      e.g. '0.3' for WR with H < 0.3 on surface\n",
    "# [thres_RL] is the threshold for Roche Lobe filling binaries\n",
    "# [thres_wind] is the threshold for wind fed binaries\n",
    "\n",
    "\n",
    "Nsim,Z,SN= '1mln','02', 'del'                                    # to select a set of simulations\n",
    "version = '3.0.0-Spindevel'                                      # select SEVN2 version\n",
    "path_results = f'./v_{version}/{Nsim}_Z{Z}_{SN}/'                # path to new folder with all useful results\n",
    "n = 4                                                            # cores for dask computation\n",
    "Hsup,thres_RL,thres_wind = 0.3,1.,0.8                            # for functions inside WRBH_select.py\n",
    "observed1, observed2, observed3 = 'cyg_x-3--Zd13', 'cyg_x-3--An22', 'cyg_x-3--Ko17'  # types of mass ranges\n",
    "\n",
    "# physical costants\n",
    "H0=14000                # Hubble time in Myr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ec8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make sure to have reduced the output_0.csv file ###\n",
    "start=time.time()\n",
    "\n",
    "read_cond = True\n",
    "if read_cond == True:\n",
    "    print('Called Read function from read.py')\n",
    "    read.Read(version,Nsim,Z,SN,Hsup,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67a2b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read with the reduced dataframes  ###\n",
    "path_df = f'{path_results}dataframes/'\n",
    "\n",
    "evolved = pd.read_csv(f'{path_df}evolved.csv')\n",
    "WRBH = dd.read_csv(f'{path_df}WRBH.csv',blocksize= 128* 1024 * 1024)\n",
    "BHBH = dd.read_csv(f'{path_df}BHBH.csv',blocksize= 128* 1024 * 1024)\n",
    "BHNS = dd.read_csv(f'{path_df}BHNS.csv',blocksize= 128* 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a12c4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep only first and last timestep for each ID to reduce ram occupation ###\n",
    "# use the function inside the WRBH_select.py file\n",
    "# set the optional argument dask = True to allow dask computation (disabled by default)\n",
    "\n",
    "WRBH = WRBH_select.first_last_ID(WRBH,dask=True)\n",
    "BHBH = WRBH_select.first_last_ID(BHBH,dask=True)\n",
    "BHNS = WRBH_select.first_last_ID(BHNS,dask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc9f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin extracting all the sub-dataframes\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "\n",
    "### Subsets of WRBH dataframe, selected with functions in WRBH_select.py ###\n",
    "startsub = time.time()\n",
    "print('Begin extracting all the sub-dataframes')\n",
    "\n",
    "### Pure spectroscopic WR (no He-naked yet) ###\n",
    "WRBH_sp = WRBH_select.WR_spectro(path_df,n,Hsup,firstlast=True)\n",
    "\n",
    "### RL filling and wind fed WRBH binaries ###\n",
    "WRBH_RL = WRBH_select.RL_filling(path_df,n,thres_min=thres_RL,firstlast=True)\n",
    "WRBH_wind = WRBH_select.RL_wind(path_df,n,thres_min=thres_wind,thres_max=thres_RL,firstlast=True)\n",
    "\n",
    "### Select candidate binaries to compare with observed known binary ###\n",
    "WRBH_obs1 = WRBH_select.obs(path_df,n,Hsup=Hsup,observed=observed1,firstlast=True) \n",
    "WRBH_obs2 = WRBH_select.obs(path_df,n,Hsup=Hsup,observed=observed2,firstlast=True) \n",
    "WRBH_obs3 = WRBH_select.obs(path_df,n,Hsup=Hsup,observed=observed3,firstlast=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c158750",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'startsub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46657/150119500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mendsub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'End extracting all the sub-dataframes in [s] '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendsub\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstartsub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'startsub' is not defined"
     ]
    }
   ],
   "source": [
    "### Subsets of binaries of compact object ###\n",
    "\n",
    "### BBH types ###\n",
    "BHBH_bound=BHBH[~np.isnan(BHBH['Semimajor'])]           # BBH gravitationally bound\n",
    "BHBH_GW=BHBH_bound.loc[BHBH_bound['GWtime'] <=H0]       # BBH that merge within Hubble time\n",
    "\n",
    "### BHNS types ###\n",
    "BHNS_bound=BHNS[~np.isnan(BHNS['Semimajor'])]           # BHNS gravitationally bound\n",
    "BHNS_GW=BHNS_bound.loc[BHNS_bound['GWtime'] <=H0]       # BHNS that merge within Hubble time\n",
    "\n",
    "endsub=time.time()\n",
    "print(f'End extracting all the sub-dataframes in [s] ', endsub-startsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20e094be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### store values of binaries with common properties ###\n",
    "\n",
    "### prepare lists ###\n",
    "df_list1 = [BHBH,BHBH_bound,BHBH_GW,BHNS,BHNS_bound,BHNS_GW]\n",
    "df_list2 = [WRBH,WRBH_sp,WRBH_RL,WRBH_wind,WRBH_obs1,WRBH_obs2,WRBH_obs3]\n",
    "\n",
    "name_list1 = ['BHBH','BHBH_bound','BHBH_GW','BHNS','BHNS_bound','BHNS_GW']\n",
    "name_list2 = ['WRBH','WRBH_sp','WRBH_RL','WRBH_wind',f'WRBH_{observed1}',f'WRBH_{observed2}',f'WRBH_{observed3}']\n",
    "\n",
    "word_list = [' BHBH ', ' bound BBH ', ' GW-BBH ', ' BHNS ', ' bound BHNS ', ' GW-BHNS ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a061344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to write the useful dataframes\n",
      "Time to write the useful dataframes [s]:  18.853395223617554\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19327/287099376.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m### output messages to check ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time to elaborate all the SEVN output files [s]: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The SEVN output files have been correctly analyzed and stored in {path_df}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "### write dataframes and useful results ###\n",
    "print('Begin to write the useful dataframes')\n",
    "startwrite=time.time()\n",
    "with open(f'{path_df}results.txt', 'w') as f:\n",
    "    f.write(f'Number of generated binaries: {Nsim} \\n')\n",
    "    f.write('Number of simulated binaries: ' + str(len(evolved.index)) + '\\n')\n",
    "    f.write('Number of WRBH systems: ' + str(len(WRBH.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write('Number of WRBH_sp spectroscopic systems: ' + str(len(WRBH_sp.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write('Number of WRBH_RL filling systems: ' + str(len(WRBH_RL.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write('Number of WRBH_wind fed systems: ' + str(len(WRBH_wind.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write(f'Number of {observed1} candidates: ' + str(len(WRBH_obs1.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write(f'Number of {observed2} candidates: ' + str(len(WRBH_obs2.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write(f'Number of {observed3} candidates: ' + str(len(WRBH_obs3.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "\n",
    "    for df1,name1,word in zip(df_list1,name_list1,word_list):\n",
    "        print(f'Begin to write the useful dataframes with {word}')\n",
    "        f.write('----------------------------------------\\n')\n",
    "        f.write(f'Number of{word}systems: ' + str(len(df1.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "        \n",
    "        # select progenitors and remnants\n",
    "        remnant = df1.drop_duplicates(['ID'],keep='last')\n",
    "        IDs = set(remnant.ID.to_list())  # list of IDs\n",
    "        progenitor = evolved.query('ID in @IDs')\n",
    "\n",
    "        # write dataframes\n",
    "        progenitor.to_csv(f'{path_df}/progenitors/p_{name1}.csv')\n",
    "        remnant.to_csv(f'{path_df}/remnants/r_{name1}.csv')\n",
    "\n",
    "        for df2,name2 in zip(df_list2,name_list2):    \n",
    "            # ID of binaries that belong in both categories\n",
    "            binID = set(df1['ID']).intersection(df2['ID'])\n",
    "\n",
    "            # progenitors and remnants of such binaries\n",
    "            prog = evolved.query('ID in @binID')\n",
    "            remnant_timesteps = df1.query('ID in @binID')\n",
    "            rem = remnant_timesteps.drop_duplicates('ID', keep='last')\n",
    "            \n",
    "            # initial and final timestamps of WRBH phase evolution for each subgroup\n",
    "            timesteps = df2.query('ID in @binID')\n",
    "            initial = timesteps.drop_duplicates('ID', keep='first')\n",
    "            final = timesteps.drop_duplicates('ID', keep='last')\n",
    "\n",
    "            # write dataframes\n",
    "            prog.to_csv(f'{path_df}/progenitors/p_{name1}_{name2}.csv')\n",
    "            rem.to_csv(f'{path_df}/remnants/r_{name1}_{name2}.csv')\n",
    "            \n",
    "            initial.to_csv(f'{path_df}/initial_WRBH/i_{name1}_{name2}.csv')\n",
    "            final.to_csv(f'{path_df}/final_WRBH/f_{name1}_{name2}.csv')\n",
    "\n",
    "            # write useful results\n",
    "            f.write(f'Number of{word}systems formed after {name2}: ' + str(len(binID)) + '\\n')\n",
    "\n",
    "endwrite=time.time()\n",
    "print('Time to write the useful dataframes [s]: ', endwrite-startwrite)\n",
    "\n",
    "### output messages to check ###\n",
    "end=time.time()\n",
    "print('Time to elaborate all the SEVN output files [s]: ', end-start)\n",
    "print(f'The SEVN output files have been correctly analyzed and stored in {path_df}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
