{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbb316",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries ###\n",
    "import os\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import WRBH_select2    # WRBH_select2.py, in the same folder, to select subset of binaries from WRBH df\n",
    "import read2           # read2.py, to reduce the original output dataframe\n",
    "\n",
    "\n",
    "### Define parameters ###\n",
    "# [version] is the SEVN2 version adopted                            e.g. '3.0.0-Spindevel'\n",
    "# [Nsim] is the number of simulations                               e.g. '1mln'\n",
    "# [Z] is the metallicity of the stars                               e.g. '02' for Z=0.02 solar metallicity\n",
    "# [SN] is the SN explosion prescription adopted                     e.g. 'del' for delayed or 'com' for compact\n",
    "# [n] number of cores involved in dask computation                  e.g.  4\n",
    "# [Hsup] is the threshold for spectroscopic WR                      e.g. '0.3' for WR with H < 0.3 on surface\n",
    "# [thres_RL] is the threshold for Roche Lobe filling binaries\n",
    "# [thres_wind] is the threshold for wind fed binaries\n",
    "# [ppisn] if the debug includes only ppisn, also or excludes it     e.g. 'with','without','only'\n",
    "\n",
    "\n",
    "Nsim,Z,SN= '1mln','015', 'com'                                    # to select a set of simulations\n",
    "kick = 'unified265'\n",
    "version = '3.0.0-Spindevel_RLO'                                      # select SEVN2 version\n",
    "path_results = f'./v_{version}/{Nsim}_Z{Z}_{SN}_{kick}/'                # path to new folder with all useful results\n",
    "n = 4                                                            # cores for dask computation\n",
    "ppisn = 'only'\n",
    "Hsup,thres_RL,thres_wind = 0.3,1.,0.8                            # for functions inside WRBH_select.py\n",
    "observed1, observed2 = 'cyg_x-3--Zd13', 'cyg_x-3--Ko17'  # types of mass ranges\n",
    "\n",
    "# physical costants\n",
    "H0=14000                # Hubble time in Myr\n",
    "\n",
    "\n",
    "### Make sure to have reduced the output_0.csv file ###\n",
    "start=time.time()\n",
    "\n",
    "read_cond = True\n",
    "if read_cond == True:\n",
    "    print('Called Read function from read2.py')\n",
    "    read2.Read(version,Nsim,Z,SN,kick,n, ppisn)\n",
    "\n",
    "\n",
    "\n",
    "### Read with the reduced dataframes  ###\n",
    "path_df = f'{path_results}ppisn_{ppisn}/'\n",
    "\n",
    "evolved = pd.read_csv(f'{path_df}evolved.csv')\n",
    "WRBH = dd.read_csv(f'{path_df}WRBH.csv',blocksize= 128* 1024 * 1024)\n",
    "BHBH = dd.read_csv(f'{path_df}BHBH.csv',blocksize= 128* 1024 * 1024)\n",
    "BHNS = dd.read_csv(f'{path_df}BHNS.csv',blocksize= 128* 1024 * 1024)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Keep only first and last timestep for each ID to reduce ram occupation ###\n",
    "# use the function inside the WRBH_select2.py file\n",
    "# set the optional argument dask = True to allow dask computation (disabled by default)\n",
    "\n",
    "WRBH = WRBH_select2.first_last_ID(WRBH,dask=True)\n",
    "BHBH = WRBH_select2.first_last_ID(BHBH,dask=True)\n",
    "BHNS = WRBH_select2.first_last_ID(BHNS,dask=True)\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "### Subsets of WRBH dataframe, selected with functions in WRBH_select2.py ###\n",
    "startsub = time.time()\n",
    "print('Begin extracting all the sub-dataframes')\n",
    "\n",
    "### Pure spectroscopic WR (no He-naked yet) ###\n",
    "WRBH_sp = WRBH_select2.WR_spectro(path_df,n,Hsup,firstlast=True)\n",
    "\n",
    "### RL filling and wind fed WRBH binaries ###\n",
    "WRBH_RL = WRBH_select2.RL_filling(path_df,n,thres_min=thres_RL,firstlast=True)\n",
    "WRBH_wind = WRBH_select2.RL_wind(path_df,n,thres_min=thres_wind,thres_max=thres_RL,firstlast=True)\n",
    "\n",
    "### Select candidate binaries to compare with observed known binary ###\n",
    "WRBH_obs1 = WRBH_select2.obs(path_df,n,Hsup=Hsup,observed=observed1,firstlast=True) \n",
    "WRBH_obs2 = WRBH_select2.obs(path_df,n,Hsup=Hsup,observed=observed2,firstlast=True) \n",
    "\n",
    "\n",
    "### Subsets of binaries of compact object ###\n",
    "\n",
    "### BBH types ###\n",
    "BHBH_bound=BHBH[~np.isnan(BHBH['Semimajor'])]           # BBH gravitationally bound\n",
    "BHBH_GW=BHBH_bound.loc[BHBH_bound['GWtime'] <=H0]       # BBH that merge within Hubble time\n",
    "\n",
    "### BHNS types ###\n",
    "BHNS_bound=BHNS[~np.isnan(BHNS['Semimajor'])]           # BHNS gravitationally bound\n",
    "BHNS_GW=BHNS_bound.loc[BHNS_bound['GWtime'] <=H0]       # BHNS that merge within Hubble time\n",
    "\n",
    "endsub=time.time()\n",
    "print(f'End extracting all the sub-dataframes in [s] ', endsub-startsub)\n",
    "\n",
    "\n",
    "### store values of binaries with common properties ###\n",
    "\n",
    "### prepare lists ###\n",
    "df_list1 = [BHBH,BHBH_bound,BHBH_GW,BHNS,BHNS_bound,BHNS_GW]\n",
    "df_list2 = [WRBH,WRBH_sp,WRBH_RL,WRBH_wind,WRBH_obs1,WRBH_obs2]\n",
    "\n",
    "name_list1 = ['BHBH','BHBH_bound','BHBH_GW','BHNS','BHNS_bound','BHNS_GW']\n",
    "name_list2 = ['WRBH','WRBH_sp','WRBH_RL','WRBH_wind',f'WRBH_{observed1}',f'WRBH_{observed2}']\n",
    "\n",
    "word_list = [' BHBH ', ' bound BBH ', ' GW-BBH ', ' BHNS ', ' bound BHNS ', ' GW-BHNS ']\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "### write dataframes and useful results ###\n",
    "print('Begin to write the useful dataframes')\n",
    "startwrite=time.time()\n",
    "with open(f'{path_df}results.txt', 'w') as f:\n",
    "    f.write(f'Number of generated binaries: {Nsim} \\n')\n",
    "    f.write('Number of simulated binaries: ' + str(len(evolved.index)) + '\\n')\n",
    "    f.write('Number of WRBH systems: ' + str(len(WRBH.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write('Number of WRBH_sp spectroscopic systems: ' + str(len(WRBH_sp.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write('Number of WRBH_RL filling systems: ' + str(len(WRBH_RL.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write('Number of WRBH_wind fed systems: ' + str(len(WRBH_wind.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write(f'Number of {observed1} candidates: ' + str(len(WRBH_obs1.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "    f.write(f'Number of {observed2} candidates: ' + str(len(WRBH_obs2.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "\n",
    "\n",
    "    for df1,name1,word in zip(df_list1,name_list1,word_list):\n",
    "        print(f'Begin to write the useful dataframes with {word}')\n",
    "        f.write('----------------------------------------\\n')\n",
    "        f.write(f'Number of{word}systems: ' + str(len(df1.drop_duplicates(subset='ID').index)) + '\\n')\n",
    "        \n",
    "        # select progenitors and remnants\n",
    "        remnant = df1.drop_duplicates(['ID'],keep='last')\n",
    "        IDs = set(remnant.ID.to_list())  # list of IDs\n",
    "        progenitor = evolved.query('ID in @IDs')\n",
    "\n",
    "        # write dataframes\n",
    "        progenitor.to_csv(f'{path_df}/progenitors/p_{name1}.csv')\n",
    "        remnant.to_csv(f'{path_df}/remnants/r_{name1}.csv')\n",
    "\n",
    "        for df2,name2 in zip(df_list2,name_list2):    \n",
    "            # ID of binaries that belong in both categories\n",
    "            binID = set(df1['ID']).intersection(df2['ID'])\n",
    "\n",
    "            # progenitors and remnants of such binaries\n",
    "            prog = evolved.query('ID in @binID')\n",
    "            remnant_timesteps = df1.query('ID in @binID')\n",
    "            rem = remnant_timesteps.drop_duplicates('ID', keep='last')\n",
    "            \n",
    "            # initial and final timestamps of WRBH phase evolution for each subgroup\n",
    "            timesteps = df2.query('ID in @binID')\n",
    "            initial = timesteps.drop_duplicates('ID', keep='first')\n",
    "            final = timesteps.drop_duplicates('ID', keep='last')\n",
    "\n",
    "            # write dataframes\n",
    "            prog.to_csv(f'{path_df}/progenitors/p_{name1}_{name2}.csv')\n",
    "            rem.to_csv(f'{path_df}/remnants/r_{name1}_{name2}.csv')\n",
    "            \n",
    "            initial.to_csv(f'{path_df}/initial_WRBH/i_{name1}_{name2}.csv')\n",
    "            final.to_csv(f'{path_df}/final_WRBH/f_{name1}_{name2}.csv')\n",
    "\n",
    "            # write useful results\n",
    "            f.write(f'Number of{word}systems formed after {name2}: ' + str(len(binID)) + '\\n')\n",
    "\n",
    "endwrite=time.time()\n",
    "print('Time to write the useful dataframes [s]: ', endwrite-startwrite)\n",
    "\n",
    "### output messages to check ###\n",
    "end=time.time()\n",
    "print('Time to elaborate all the SEVN output files [s]: ', end-start)\n",
    "print(f'The SEVN output files have been correctly analyzed and stored in {path_df}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
